{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f91391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ìµœì‹  ë°ì´í„° íŒŒì¼: munpia_contest_ranking_20250605_225739.json\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: 200ê°œ ì‘í’ˆ\n",
      "ğŸ“‹ ì»¬ëŸ¼: ['crawl_rank', 'novel_url', 'novel_id', 'rank_number', 'author', 'title', 'genre', 'view_count', 'view_count_number', 'rank_change', 'rank_change_number', 'reading_rate', 'reading_rate_number']\n",
      "\n",
      "ğŸ“Š ê¸°ë³¸ í†µê³„:\n",
      "   ì œëª©ì´ ìˆëŠ” ì‘í’ˆ: 200ê°œ\n",
      "   ì¡°íšŒìˆ˜ê°€ ìˆëŠ” ì‘í’ˆ: 200ê°œ\n",
      "   ì—°ë…ë¥ ì´ ìˆëŠ” ì‘í’ˆ: 152ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ ë°ì´í„° íŒŒì¼ ì°¾ê¸° ë° ë¡œë“œ\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ê°€ì¥ ìµœì‹  JSON íŒŒì¼ ì°¾ê¸°\n",
    "json_files = glob.glob('munpia_contest_ranking_*.json')\n",
    "if json_files:\n",
    "    latest_json = max(json_files, key=lambda x: x.split('_')[-1].replace('.json', ''))\n",
    "    print(f\"ğŸ“ ìµœì‹  ë°ì´í„° íŒŒì¼: {latest_json}\")\n",
    "    \n",
    "    # JSON ë°ì´í„° ë¡œë“œ\n",
    "    with open(latest_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # DataFrame ìƒì„±\n",
    "    df = pd.DataFrame(data['rankings'])\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ì‘í’ˆ\")\n",
    "    print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "    \n",
    "    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“Š ê¸°ë³¸ í†µê³„:\")\n",
    "    print(f\"   ì œëª©ì´ ìˆëŠ” ì‘í’ˆ: {df['title'].notna().sum()}ê°œ\")\n",
    "    print(f\"   ì¡°íšŒìˆ˜ê°€ ìˆëŠ” ì‘í’ˆ: {df['view_count_number'].notna().sum()}ê°œ\")\n",
    "    print(f\"   ì—°ë…ë¥ ì´ ìˆëŠ” ì‘í’ˆ: {df['reading_rate_number'].notna().sum()}ê°œ\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì €ì¥ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   ë¨¼ì € íŒŒì„œë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a58406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "class MunpiaImprovedParser:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://novel.munpia.com/page/hd.contest2025/group/contest/view/a/contest/list\"\n",
    "        self.html_file_path = r\"E:\\ì¥ë‚œê° ë¬¸í”¼ì•„\\ë¬¸í”¼ì•„0611.html\"\n",
    "\n",
    "    def parse_local_file(self):\n",
    "        \"\"\"ë¡œì»¬ HTML íŒŒì¼ íŒŒì‹±\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.html_file_path):\n",
    "                print(f\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.html_file_path}\")\n",
    "                return None\n",
    "\n",
    "            print(f\"ğŸ“ ë¡œì»¬ íŒŒì¼ ì½ê¸°: {self.html_file_path}\")\n",
    "\n",
    "            # ì¸ì½”ë”© ì²˜ë¦¬\n",
    "            html_content = None\n",
    "            for encoding in ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']:\n",
    "                try:\n",
    "                    with open(self.html_file_path, 'r', encoding=encoding) as f:\n",
    "                        html_content = f.read()\n",
    "                    print(f\"âœ… ì¸ì½”ë”© ì„±ê³µ: {encoding}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if not html_content:\n",
    "                print(\"âŒ íŒŒì¼ ì¸ì½”ë”© ì‹¤íŒ¨\")\n",
    "                return None\n",
    "\n",
    "            print(f\"ğŸ“„ HTML í¬ê¸°: {len(html_content):,} ë¬¸ì\")\n",
    "\n",
    "            return self.parse_html_content(html_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html_content(self, html_content):\n",
    "        \"\"\"HTML ë‚´ìš© íŒŒì‹±\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ” HTML íŒŒì‹± ì‹œì‘...\")\n",
    "\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # í˜ì´ì§€ ì œëª© í™•ì¸\n",
    "            title = soup.find('title')\n",
    "            if title:\n",
    "                print(f\"í˜ì´ì§€ ì œëª©: {title.get_text()}\")\n",
    "\n",
    "            rankings = self.extract_rankings(soup)\n",
    "\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            result = {\n",
    "                'collection_time': current_time,\n",
    "                'total_works': len(rankings),\n",
    "                'rankings': rankings,\n",
    "                'url': self.url,\n",
    "                'method': 'local_file_parsing'\n",
    "            }\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ HTML íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def extract_rankings(self, soup):\n",
    "        \"\"\"ê°œì„ ëœ ë­í‚¹ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        rankings = []\n",
    "\n",
    "        try:\n",
    "            print(\"ğŸ” ë­í‚¹ ë°ì´í„° ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "            # best-rank-list-display ì»¨í…Œì´ë„ˆ ì°¾ê¸°\n",
    "            rank_container = soup.find('div', {'id': 'best-rank-list-display'})\n",
    "            if not rank_container:\n",
    "                print(\"âŒ best-rank-list-display ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "                return []\n",
    "\n",
    "            print(\"âœ… best-rank-list-display ì»¨í…Œì´ë„ˆ ë°œê²¬\")\n",
    "\n",
    "            # ì†Œì„¤ ë§í¬ë“¤ ì°¾ê¸° (munpia.com/ìˆ«ì íŒ¨í„´)\n",
    "            novel_links = []\n",
    "            all_links = rank_container.find_all('a', href=True)\n",
    "\n",
    "            for link in all_links:\n",
    "                href = link.get('href', '')\n",
    "                # novel.munpia.com/ìˆ«ì íŒ¨í„´ í™•ì¸\n",
    "                if re.search(r'novel\\.munpia\\.com/\\d{6,}', href):\n",
    "                    novel_links.append(link)\n",
    "\n",
    "            print(f\"ğŸ“š ë°œê²¬ëœ ì†Œì„¤ ë§í¬: {len(novel_links)}ê°œ\")\n",
    "\n",
    "            if not novel_links:\n",
    "                print(\"âŒ ìœ íš¨í•œ ì†Œì„¤ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "                return []\n",
    "\n",
    "            # ê° ì†Œì„¤ ë§í¬ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "            for idx, link in enumerate(novel_links, 1):\n",
    "                try:\n",
    "                    rank_data = self.extract_single_item(link, idx)\n",
    "                    if rank_data:\n",
    "                        rankings.append(rank_data)\n",
    "\n",
    "                        # ì²˜ìŒ 20ê°œ ë¯¸ë¦¬ë³´ê¸°\n",
    "                        if idx <= 20:\n",
    "                            rank_num = rank_data.get('rank_number', '?')\n",
    "                            title = rank_data.get('title', 'N/A')[:30]\n",
    "                            author = rank_data.get('author', 'N/A')[:15]\n",
    "                            view_count = rank_data.get('view_count', 'N/A')\n",
    "                            genre = rank_data.get('genre', 'N/A')[:20]\n",
    "                            reading_rate = rank_data.get('reading_rate', 'N/A')\n",
    "\n",
    "                            print(f\"{rank_num:>3}. {title:<30} | {author:<15} | {view_count:>8} | {reading_rate:>7} | {genre}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"í•­ëª© {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"âœ… ì´ {len(rankings)}ê°œ ì‘í’ˆ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ\")\n",
    "            return rankings\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë­í‚¹ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "    def extract_single_item(self, item, idx):\n",
    "        \"\"\"ë‹¨ì¼ í•­ëª© ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            # URL ì¶”ì¶œ ë° ì •ê·œí™”\n",
    "            novel_url = item.get('href', '')\n",
    "            if not novel_url.startswith('http'):\n",
    "                novel_url = 'https://' + novel_url if novel_url.startswith('novel.munpia.com') else 'https://novel.munpia.com' + novel_url\n",
    "\n",
    "            # ì†Œì„¤ ID ì¶”ì¶œ\n",
    "            novel_id_match = re.search(r'novel\\.munpia\\.com/(\\d+)', novel_url)\n",
    "            novel_id = novel_id_match.group(1) if novel_id_match else None\n",
    "\n",
    "            rank_data = {\n",
    "                'crawl_rank': idx,\n",
    "                'novel_url': novel_url,\n",
    "                'novel_id': novel_id\n",
    "            }\n",
    "\n",
    "            # ì§ì ‘ ìì‹ divë“¤ì—ì„œ í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„° ì¶”ì¶œ\n",
    "            direct_divs = item.find_all('div', recursive=False)\n",
    "\n",
    "            for div in direct_divs:\n",
    "                div_classes = div.get('class', [])\n",
    "                if not div_classes:\n",
    "                    continue\n",
    "\n",
    "                class_name = ' '.join(div_classes)\n",
    "\n",
    "                # ìˆœìœ„ ë²ˆí˜¸\n",
    "                if 'num' in class_name:\n",
    "                    rank_data['rank_number'] = self.clean_text(div.get_text())\n",
    "\n",
    "                # ì‘ê°€ëª…\n",
    "                elif 'author' in class_name:\n",
    "                    rank_data['author'] = self.clean_text(div.get_text())\n",
    "\n",
    "                # ì œëª©\n",
    "                elif 'title' in class_name:\n",
    "                    title_span = div.find('span', class_='title-wrap')\n",
    "                    if title_span:\n",
    "                        rank_data['title'] = self.clean_text(title_span.get_text())\n",
    "                    else:\n",
    "                        # ë‹¤ë¥¸ spanì´ë‚˜ í…ìŠ¤íŠ¸ í™•ì¸\n",
    "                        span = div.find('span')\n",
    "                        if span:\n",
    "                            rank_data['title'] = self.clean_text(span.get_text())\n",
    "                        else:\n",
    "                            rank_data['title'] = self.clean_text(div.get_text())\n",
    "\n",
    "                # ì¥ë¥´\n",
    "                elif 'genre' in class_name:\n",
    "                    span = div.find('span')\n",
    "                    if span:\n",
    "                        rank_data['genre'] = self.clean_text(span.get_text())\n",
    "                    else:\n",
    "                        rank_data['genre'] = self.clean_text(div.get_text())\n",
    "\n",
    "                # ì¡°íšŒìˆ˜\n",
    "                elif 'view-count' in class_name:\n",
    "                    view_text = self.clean_text(div.get_text())\n",
    "                    rank_data['view_count'] = view_text\n",
    "                    rank_data['view_count_number'] = self.extract_number(view_text)\n",
    "\n",
    "                # ìˆœìœ„ ë³€ë™\n",
    "                elif 'rank-changes' in class_name:\n",
    "                    # SVG íƒœê·¸ ì œê±°\n",
    "                    for svg in div.find_all('svg'):\n",
    "                        svg.decompose()\n",
    "\n",
    "                    change_text = self.clean_text(div.get_text())\n",
    "                    rank_data['rank_change'] = change_text\n",
    "\n",
    "                    # ìˆ«ì ë³€ë™ ì¶”ì¶œ\n",
    "                    if change_text and re.match(r'^[+-]?\\d+$', change_text.strip()):\n",
    "                        try:\n",
    "                            rank_data['rank_change_number'] = int(change_text.strip())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # ì—°ë…ë¥ \n",
    "                elif 'percent' in class_name:\n",
    "                    span = div.find('span')\n",
    "                    if span:\n",
    "                        reading_rate = self.clean_text(span.get_text())\n",
    "                    else:\n",
    "                        reading_rate = self.clean_text(div.get_text())\n",
    "\n",
    "                    rank_data['reading_rate'] = reading_rate\n",
    "\n",
    "                    # ìˆ«ìë¡œ ë³€í™˜\n",
    "                    if reading_rate and '%' in reading_rate:\n",
    "                        try:\n",
    "                            rate_num = float(reading_rate.replace('%', '').strip())\n",
    "                            if 0 <= rate_num <= 100:\n",
    "                                rank_data['reading_rate_number'] = rate_num\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            # í•„ìˆ˜ í•„ë“œ í™•ì¸\n",
    "            if rank_data.get('novel_id') and (rank_data.get('title') or rank_data.get('author')):\n",
    "                return rank_data\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ë¦¬\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        text = re.sub(r'\\s+', ' ', str(text))\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        text = text.replace('\\u200b', '')\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def extract_number(self, text):\n",
    "        \"\"\"ìˆ«ì ì¶”ì¶œ\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "\n",
    "        # ì½¤ë§ˆê°€ í¬í•¨ëœ ìˆ«ì íŒ¨í„´\n",
    "        numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*', text)\n",
    "\n",
    "        if numbers:\n",
    "            try:\n",
    "                # ê°€ì¥ í° ìˆ«ì ë°˜í™˜\n",
    "                max_num = 0\n",
    "                for num_str in numbers:\n",
    "                    try:\n",
    "                        num = int(num_str.replace(',', ''))\n",
    "                        if num > max_num:\n",
    "                            max_num = num\n",
    "                    except:\n",
    "                        continue\n",
    "                return max_num\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def save_data(self, data):\n",
    "        \"\"\"ë°ì´í„° ì €ì¥\"\"\"\n",
    "        if not data or not data.get('rankings'):\n",
    "            print(\"âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "            # JSON ì €ì¥\n",
    "            json_filename = f\"munpia_contest_ranking_{timestamp}.json\"\n",
    "            with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # CSV ì €ì¥\n",
    "            df = pd.DataFrame(data['rankings'])\n",
    "            csv_filename = f\"munpia_contest_ranking_{timestamp}.csv\"\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "            print(f\"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ:\")\n",
    "            print(f\"   ğŸ“„ JSON: {json_filename}\")\n",
    "            print(f\"   ğŸ“Š CSV: {csv_filename}\")\n",
    "\n",
    "            # ìƒì„¸ í†µê³„\n",
    "            rankings = data['rankings']\n",
    "            print(f\"\\nğŸ“ˆ ìƒì„¸ í†µê³„:\")\n",
    "            print(f\"   ì´ ì‘í’ˆ ìˆ˜: {len(rankings)}\")\n",
    "\n",
    "            if rankings:\n",
    "                # í•„ë“œë³„ ì¶”ì¶œ ì„±ê³µë¥ \n",
    "                fields_stats = {\n",
    "                    'novel_id': len([r for r in rankings if r.get('novel_id')]),\n",
    "                    'title': len([r for r in rankings if r.get('title')]),\n",
    "                    'author': len([r for r in rankings if r.get('author')]),\n",
    "                    'genre': len([r for r in rankings if r.get('genre')]),\n",
    "                    'view_count': len([r for r in rankings if r.get('view_count')]),\n",
    "                    'reading_rate': len([r for r in rankings if r.get('reading_rate')]),\n",
    "                    'rank_change': len([r for r in rankings if r.get('rank_change')])\n",
    "                }\n",
    "\n",
    "                print(\"   í•„ë“œë³„ ì¶”ì¶œ ì„±ê³µ:\")\n",
    "                for field, count in fields_stats.items():\n",
    "                    percentage = (count / len(rankings)) * 100\n",
    "                    print(f\"     {field}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "                # ì¡°íšŒìˆ˜ í†µê³„\n",
    "                view_counts = [r.get('view_count_number', 0) for r in rankings if r.get('view_count_number', 0) > 0]\n",
    "                if view_counts:\n",
    "                    print(f\"\\n   ì¡°íšŒìˆ˜ í†µê³„:\")\n",
    "                    print(f\"     í‰ê· : {sum(view_counts)/len(view_counts):,.0f}\")\n",
    "                    print(f\"     ìµœê³ : {max(view_counts):,}\")\n",
    "                    print(f\"     ìµœì €: {min(view_counts):,}\")\n",
    "\n",
    "                # ë…ì„œìœ¨ í†µê³„\n",
    "                reading_rates = [r.get('reading_rate_number', 0) for r in rankings if r.get('reading_rate_number', 0) > 0]\n",
    "                if reading_rates:\n",
    "                    print(f\"\\n   ë…ì„œìœ¨ í†µê³„:\")\n",
    "                    print(f\"     í‰ê· : {sum(reading_rates)/len(reading_rates):.2f}%\")\n",
    "                    print(f\"     ìµœê³ : {max(reading_rates):.2f}%\")\n",
    "                    print(f\"     ìµœì €: {min(reading_rates):.2f}%\")\n",
    "\n",
    "            print(f\"\\nğŸ’¾ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "            print(f\"   {os.path.abspath(json_filename)}\")\n",
    "            print(f\"   {os.path.abspath(csv_filename)}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6884d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë¬¸í”¼ì•„ ê°œì„ ëœ ë¡œì»¬ íŒŒì¼ íŒŒì„œ ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ ê°œì„ ì‚¬í•­:\n",
      "   âœ… ë¡œì»¬ HTML íŒŒì¼ ì§ì ‘ ì½ê¸°\n",
      "   âœ… ì •í™•í•œ HTML êµ¬ì¡° ë¶„ì„\n",
      "   âœ… í•„ë“œë³„ ì •í™•í•œ ì¶”ì¶œ\n",
      "   âœ… ì¡°íšŒìˆ˜, ì—°ë…ë¥  ìˆ«ì ë³€í™˜\n",
      "   âœ… ìƒì„¸ í†µê³„ ì œê³µ\n",
      "\n",
      "ğŸ“ íŒŒì¼ ê²½ë¡œ: E:\\ì¥ë‚œê° ë¬¸í”¼ì•„\\ë¬¸í”¼ì•„0611.html\n",
      "\n",
      "ğŸ“ ë¡œì»¬ íŒŒì¼ ì½ê¸°: E:\\ì¥ë‚œê° ë¬¸í”¼ì•„\\ë¬¸í”¼ì•„0611.html\n",
      "âœ… ì¸ì½”ë”© ì„±ê³µ: cp949\n",
      "ğŸ“„ HTML í¬ê¸°: 286,889 ë¬¸ì\n",
      "ğŸ” HTML íŒŒì‹± ì‹œì‘...\n",
      "ğŸ” ë­í‚¹ ë°ì´í„° ê²€ìƒ‰ ì¤‘...\n",
      "âœ… best-rank-list-display ì»¨í…Œì´ë„ˆ ë°œê²¬\n",
      "ğŸ“š ë°œê²¬ëœ ì†Œì„¤ ë§í¬: 200ê°œ\n",
      "  1. ë°©êµ¬ì„ ê²½ì œí•™ì                       | í˜ë¥´ì„¸ë¥´í¬           |   10,022 |  63.18% | í˜„ëŒ€íŒíƒ€ì§€\n",
      "  2. ë‡Œê°ì„± í›„ ì¸ìƒ ì—­ì „                    | ì—°í•¨â„¢             |    7,831 |  37.03% | í˜„ëŒ€íŒíƒ€ì§€, íŒíƒ€ì§€\n",
      "  3. ìì‚´ ë¶€ëŒ€ì˜ ë¶ˆì‚¬ìëŠ” ì°©ê°ë‹¹í•œë‹¤              | í’€ë“œë¡œìš°            |    7,437 |  63.21% | í“¨ì „, í˜„ëŒ€íŒíƒ€ì§€\n",
      "  4. ë°±ìˆ˜ê°€ ë„ˆë¬´ ìœ ëŠ¥í•˜ë‹¤.                   | ì‹œí•˜              |    7,138 |  59.31% | í˜„ëŒ€íŒíƒ€ì§€, ë“œë¼ë§ˆ\n",
      "  5. í•˜ë£¨ì— 2ë°°ì”© ê°•í•´ì§€ëŠ” í—Œí„°                | ìº¡ìŠí˜¸í…”            |    6,427 |       - | í˜„ëŒ€íŒíƒ€ì§€, í“¨ì „\n",
      "  6. ì‚¼ë¥˜ë¬´ì‚¬ì—ì„œ ì²œì–µ íˆ¬ìˆ˜ê¹Œì§€                 | ìš°ë¦¼â„¢             |    5,519 |  42.22% | ìŠ¤í¬ì¸ , í˜„ëŒ€íŒíƒ€ì§€\n",
      "  7. ì´ˆì›”ê¸‰ íˆ¬ììê°€ ëˆì„ ë„ˆë¬´ ì˜ ë²             | ëŠ‘íƒ€ë¦¬ìš”            |    5,449 |  61.73% | í˜„ëŒ€íŒíƒ€ì§€, í“¨ì „\n",
      "  8. ì•Œ ì¹´í¬ë„¤ ê²€ì€ë¨¸ë¦¬ ë°ë¦´ì‚¬ìœ„                | Merkava         |    5,417 |  65.92% | ëŒ€ì²´ì—­ì‚¬, í˜„ëŒ€íŒíƒ€ì§€\n",
      "  9. ë¬¼ë¦¬í•™ë„ëŠ” ë§ˆë²•ì´ ë„ˆë¬´ ì‰½ë‹¤                | í ë°•              |    4,932 |  61.43% | íŒíƒ€ì§€, í“¨ì „\n",
      " 10. ê·€í™˜í–ˆë”ë‹ˆ êµ­ê°€ê¶Œë ¥ê¸‰ ê±°ë¬¼ì´ ë˜ì—ˆë‹¤            | ë‹¨ì—´              |    4,098 |  33.64% | í˜„ëŒ€íŒíƒ€ì§€, íŒíƒ€ì§€\n",
      " 11. ì„±êµ° ìˆœì¢…ëŒ€ì™• ì¼ëŒ€ê¸°                    | ë¦¬ì²¼ë Œ             |    3,829 |  56.81% | ëŒ€ì²´ì—­ì‚¬, íŒíƒ€ì§€\n",
      " 12. ì·¨ë¯¸ë¡œ ë°œë¡±ë„ë¥´                       | í”¼ë…¸í‚¤í™            |    3,575 |  46.28% | ìŠ¤í¬ì¸ , í˜„ëŒ€íŒíƒ€ì§€\n",
      " 13. ì§‘êµ¬ì„ ë¡œì¼“ë°°ì†¡                       | ì•„ë¼ë§Œ             |    3,120 |  56.63% | í˜„ëŒ€íŒíƒ€ì§€, íŒíƒ€ì§€\n",
      " 14. ë‚´ ëšë°°ê¸°ì— ë‚ ì•„ë“  AI                  | ê¸€ì¥ì–´ì§œê¸°           |    2,814 |  74.97% | í˜„ëŒ€íŒíƒ€ì§€, íŒíƒ€ì§€\n",
      " 15. ì¶•êµ¬ ì²œì¬ì¸ ê±° ë‚˜ë§Œ ëª¨ë¦„                 | ì‹œì‘ì˜ë°”ë‹¤           |    2,735 |  76.72% | ìŠ¤í¬ì¸ \n",
      " 16. 2êµ° í¬ìˆ˜ê°€ 1êµ°ë³´ë‹¤ ì˜í•¨                 | ì¹˜ê·œì¹˜ê·œ            |    2,524 |  37.13% | ìŠ¤í¬ì¸ , í˜„ëŒ€íŒíƒ€ì§€\n",
      " 17. ëˆ ë²„ëŠ” ì¬ëŠ¥ì´ ë„˜ì³ë‚˜ëŠ” ì˜ì£¼ë‹˜              | ì˜¤í† ë§ˆí†¤G           |    2,455 |  64.93% | íŒíƒ€ì§€, í˜„ëŒ€íŒíƒ€ì§€\n",
      " 18. 101ë²ˆì§¸ ë“œë˜í”„í‹°ëŠ” ì•¼êµ¬ê°€ ë„ˆë¬´ ì‰¬ì›€          | ë°¤ì¹´ë‹¤             |    2,252 |  50.88% | ìŠ¤í¬ì¸ , í˜„ëŒ€íŒíƒ€ì§€\n",
      " 19. í† í† ì¶©ì´ ëˆ ë³µì‚¬ ë²„ê·¸ë¥¼ ì”€                | ê¸ì •ì ë§ˆìŒ           |    2,236 |  49.66% | í˜„ëŒ€íŒíƒ€ì§€, ìŠ¤í¬ì¸ \n",
      " 20. ìˆ˜ì˜ì‚¬, ë§ˆë¬¼ë“¤ì˜ ëª…ì˜ê°€ ë¨                | ë•…ì½©í—Œí„°            |    2,106 |  61.76% | í˜„ëŒ€íŒíƒ€ì§€, í“¨ì „\n",
      "âœ… ì´ 200ê°œ ì‘í’ˆ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ\n",
      "âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ JSON: munpia_contest_ranking_20250611_215656.json\n",
      "   ğŸ“Š CSV: munpia_contest_ranking_20250611_215656.csv\n",
      "\n",
      "ğŸ“ˆ ìƒì„¸ í†µê³„:\n",
      "   ì´ ì‘í’ˆ ìˆ˜: 200\n",
      "   í•„ë“œë³„ ì¶”ì¶œ ì„±ê³µ:\n",
      "     novel_id: 200ê°œ (100.0%)\n",
      "     title: 200ê°œ (100.0%)\n",
      "     author: 200ê°œ (100.0%)\n",
      "     genre: 200ê°œ (100.0%)\n",
      "     view_count: 200ê°œ (100.0%)\n",
      "     reading_rate: 200ê°œ (100.0%)\n",
      "     rank_change: 200ê°œ (100.0%)\n",
      "\n",
      "   ì¡°íšŒìˆ˜ í†µê³„:\n",
      "     í‰ê· : 850\n",
      "     ìµœê³ : 10,022\n",
      "     ìµœì €: 45\n",
      "\n",
      "   ë…ì„œìœ¨ í†µê³„:\n",
      "     í‰ê· : 52.83%\n",
      "     ìµœê³ : 86.10%\n",
      "     ìµœì €: 8.26%\n",
      "\n",
      "ğŸ’¾ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n",
      "   e:\\ì¥ë‚œê° ë¬¸í”¼ì•„\\munpia_contest_ranking_20250611_215656.json\n",
      "   e:\\ì¥ë‚œê° ë¬¸í”¼ì•„\\munpia_contest_ranking_20250611_215656.csv\n",
      "\n",
      "ğŸ‰ íŒŒì‹± ì™„ë£Œ! 200ê°œ ì‘í’ˆ ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥ ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "# ê°œì„ ëœ íŒŒì„œ ì‹¤í–‰\n",
    "print(\"ğŸš€ ë¬¸í”¼ì•„ ê°œì„ ëœ ë¡œì»¬ íŒŒì¼ íŒŒì„œ ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"ğŸ“‹ ê°œì„ ì‚¬í•­:\")\n",
    "print(\"   âœ… ë¡œì»¬ HTML íŒŒì¼ ì§ì ‘ ì½ê¸°\")\n",
    "print(\"   âœ… ì •í™•í•œ HTML êµ¬ì¡° ë¶„ì„\")\n",
    "print(\"   âœ… í•„ë“œë³„ ì •í™•í•œ ì¶”ì¶œ\")\n",
    "print(\"   âœ… ì¡°íšŒìˆ˜, ì—°ë…ë¥  ìˆ«ì ë³€í™˜\")\n",
    "print(\"   âœ… ìƒì„¸ í†µê³„ ì œê³µ\")\n",
    "print()\n",
    "\n",
    "parser = MunpiaImprovedParser()\n",
    "print(f\"ğŸ“ íŒŒì¼ ê²½ë¡œ: {parser.html_file_path}\")\n",
    "print()\n",
    "\n",
    "result = parser.parse_local_file()\n",
    "\n",
    "if result:\n",
    "    success = parser.save_data(result)\n",
    "    if success:\n",
    "        print(f\"\\nğŸ‰ íŒŒì‹± ì™„ë£Œ! {result['total_works']}ê°œ ì‘í’ˆ ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥ ì„±ê³µ\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ íŒŒì‹±ì€ ì„±ê³µí–ˆì§€ë§Œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ\")\n",
    "else:\n",
    "    print(\"\\nâŒ íŒŒì‹± ì‹¤íŒ¨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
